# Crawler Shein

Bem-vindo ao projeto de web scraping automatizado do site Shein! Este projeto foi desenvolvido para automatizar a extraÃ§Ã£o de dados de produtos do site Shein, armazenÃ¡-los em um banco de dados SQLite, validar os dados utilizando o Pydantic e disponibilizar uma API para gerenciar os endpoints da aplicaÃ§Ã£o. ğŸ•µï¸â€â™‚ï¸ğŸ“ˆ

## Funcionalidades

- **Web Scraping Automatizado:** Utiliza Selenium para automatizar a navegaÃ§Ã£o e extraÃ§Ã£o de dados de produtos do site Shein. ğŸŒğŸ¤–
- **Armazenamento em Banco de Dados:** Utiliza um banco de dados SQLite para armazenar os dados extraÃ­dos. ğŸ—ƒï¸ğŸ“Š
- **ValidaÃ§Ã£o de Dados com Pydantic:** Utiliza o Pydantic para validar os dados extraÃ­dos antes de armazenÃ¡-los no banco de dados. âš™ï¸ğŸ”
- **API FastAPI:** Disponibiliza uma API utilizando o FastAPI para gerenciar os endpoints da aplicaÃ§Ã£o. ğŸš€ğŸ”Œ
- **Apache Kafka:** Utiliza o Apache Kafka para permitir a comunicaÃ§Ã£o assÃ­ncrona e distribuÃ­da entre os mÃ³dulos do projeto, como o crawler, API e Banco de dados. ğŸ“¡ğŸ”—

## InstalaÃ§Ã£o

Para executar este projeto em sua mÃ¡quina local, siga os passos abaixo:

1. **Clone o repositÃ³rio**:
   ```sh
   git clone https://github.com/wesleyolvr/shein_crawler.git
   ```

2. **Crie e Ative um Ambiente Virtual**:
   - No terminal, navegue atÃ© o diretÃ³rio do seu projeto:
     ```sh
     cd /path/to/your/project
     ```
   - Crie um ambiente virtual:
     ```sh
     python -m venv venv
     ```
   - Ative o ambiente virtual:
     - No Windows:
     ```sh
       venv\Scripts\activate
     ```
     - No macOS/Linux:
     ```sh
       source venv/bin/activate
     ```

3. **Ajuste o arquivo de configuraÃ§Ã£o**:
   Renomeie o arquivo `config_sample.ini` para `config.ini` e insira as informaÃ§Ãµes do banco de dados e do Kafka conforme necessÃ¡rio.

4. **Inicie o Kafka e o Zookeeper**:
   Siga as instruÃ§Ãµes para iniciar o Apache Kafka e o Apache Zookeeper conforme documentado [aqui](https://github.com/wesleyolvr/shein_crawler/blob/feature/crawler_api_kafka/kafka-zookeeper.md).

5. **Inicie a API FastAPI e o consumidor Kafka**:
   ```sh
   python start.py
   ```
6. **Inicie o script do Crawler:**
   ```sh
   python crawler/shein_crawler.py
   ```

## Uso

Depois de seguir as etapas de instalaÃ§Ã£o, a API estarÃ¡ disponÃ­vel em `http://localhost:8000` e vocÃª pode acessar a documentaÃ§Ã£o interativa do Swagger em `http://localhost:8000/docs`.

### Endpoints disponÃ­veis:

- **`/produtos`**: Lista todos os produtos extraÃ­dos do site Shein.
- **`/produtos/{product_id}`**: Retorna um produto especÃ­fico pelo ID.

## ContribuiÃ§Ã£o

Se vocÃª deseja contribuir com melhorias para este projeto, siga as diretrizes abaixo:

1. Crie uma nova branch:
   ```sh
   git checkout -b feature-nova-funcionalidade
   ```

2. FaÃ§a suas alteraÃ§Ãµes e commit:
   ```sh
   git commit -am 'Adiciona nova funcionalidade'
   ```

3. Envie para o GitHub:
   ```sh
   git push origin feature-nova-funcionalidade
   ```

4. Crie um novo Pull Request e aguarde a revisÃ£o.


## PrÃ³ximos Passos
- **SubstituiÃ§Ã£o do Selenium pelo Scrapy:** Estou atualmente trabalhando para substituir o Selenium pelo Scrapy, a fim de aumentar a eficiÃªncia da extraÃ§Ã£o de dados e possibilitar uma raspagem de dados escalÃ¡vel com mais facilidade. Isso permitirÃ¡ a obtenÃ§Ã£o de uma maior quantidade de dados de forma mais rÃ¡pida e eficiente.
- **AnÃ¡lise de Dados:** Implementar funcionalidades para consumir os dados do banco e realizar anÃ¡lises de tendÃªncias de preÃ§os. ğŸ“‰ğŸ“Š
- **ServiÃ§o de ComparaÃ§Ã£o de PreÃ§os:** Desenvolver um serviÃ§o que compara os preÃ§os atuais dos produtos com seus histÃ³ricos para identificar oportunidades de compra. ğŸ’°ğŸ”


## LicenÃ§a

Este projeto Ã© licenciado sob a [MIT License](https://opensource.org/licenses/MIT) - veja o arquivo [LICENSE](https://github.com/seu-usuario/nome-do-projeto/blob/main/LICENSE) para mais detalhes. ğŸš€ğŸ¤
